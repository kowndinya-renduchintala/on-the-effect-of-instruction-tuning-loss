{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "asIjIy1yuHZl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_percentage_change_llama(base_model, fine_tuned_model, component_type=\"mlp\"):\n",
        "    \"\"\"\n",
        "    Calculate and plot the percentage change in weights for MLPs or attention heads across layers\n",
        "    between a base LLaMA-2 model and a fine-tuned model, with separate lines for up-projection and down-projection weights.\n",
        "\n",
        "    Args:\n",
        "        base_model: The base LLaMA-2 model.\n",
        "        fine_tuned_model: The fine-tuned LLaMA-2 model.\n",
        "        component_type (str): The type of component to analyze (\"mlp\" or \"attention\").\n",
        "\n",
        "    \"\"\"\n",
        "    assert component_type in [\"mlp\", \"attention\"], \"component_type must be 'mlp' or 'attention'.\"\n",
        "\n",
        "    # Transfer models to the available device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    base_model = base_model.to(device)\n",
        "    fine_tuned_model = fine_tuned_model.to(device)\n",
        "\n",
        "    num_layers = len(base_model.model.layers)\n",
        "    up_proj_changes = []\n",
        "    down_proj_changes = []\n",
        "    q_changes, k_changes, v_changes, o_changes = [], [], [], []\n",
        "\n",
        "\n",
        "    if component_type == \"mlp\":\n",
        "            for i in range(num_layers):\n",
        "                # Up-projection weights\n",
        "                base_up_weights = base_model.model.layers[i].mlp.up_proj.weight.data.to(device)\n",
        "                fine_tuned_up_weights = fine_tuned_model.model.layers[i].mlp.up_proj.weight.data.to(device)\n",
        "                up_change = torch.abs(fine_tuned_up_weights - base_up_weights) / torch.abs(base_up_weights + 1e-1000)\n",
        "                up_proj_changes.append(torch.mean(up_change).item() * 100)\n",
        "\n",
        "                # Down-projection weights\n",
        "                base_down_weights = base_model.model.layers[i].mlp.down_proj.weight.data.to(device)\n",
        "                fine_tuned_down_weights = fine_tuned_model.model.layers[i].mlp.down_proj.weight.data.to(device)\n",
        "                down_change = torch.abs(fine_tuned_down_weights - base_down_weights) / torch.abs(base_down_weights + 1e-1000)\n",
        "                down_proj_changes.append(torch.mean(down_change).item() * 100)\n",
        "\n",
        "            return (up_proj_changes, down_proj_changes)\n",
        "            # Plot the results\n",
        "            # plt.figure(figsize=(10, 6))\n",
        "            # plt.plot(range(num_layers), up_proj_changes, marker='o', color='blue', label=\"Up-Projection Weights\")\n",
        "            # plt.plot(range(num_layers), down_proj_changes, marker='o', color='red', label=\"Down-Projection Weights\")\n",
        "            # plt.title(\"Percentage Change in MLP Weights Across Layers (LLaMA-2)\")\n",
        "            # plt.xlabel(\"Layer\")\n",
        "            # plt.ylabel(\"Percentage Change (%)\")\n",
        "            # plt.grid(True)\n",
        "            # plt.legend()\n",
        "            # plt.show()\n",
        "\n",
        "    elif component_type == \"attention\":\n",
        "            for i in range(num_layers):\n",
        "                # Extract Q, K, V, O projection weights\n",
        "                base_q_weights = base_model.model.layers[i].self_attn.q_proj.weight.data.to(device)\n",
        "                fine_tuned_q_weights = fine_tuned_model.model.layers[i].self_attn.q_proj.weight.data.to(device)\n",
        "                base_k_weights = base_model.model.layers[i].self_attn.k_proj.weight.data.to(device)\n",
        "                fine_tuned_k_weights = fine_tuned_model.model.layers[i].self_attn.k_proj.weight.data.to(device)\n",
        "                base_v_weights = base_model.model.layers[i].self_attn.v_proj.weight.data.to(device)\n",
        "                fine_tuned_v_weights = fine_tuned_model.model.layers[i].self_attn.v_proj.weight.data.to(device)\n",
        "                base_o_weights = base_model.model.layers[i].self_attn.o_proj.weight.data.to(device)\n",
        "                fine_tuned_o_weights = fine_tuned_model.model.layers[i].self_attn.o_proj.weight.data.to(device)\n",
        "\n",
        "                # Compute percentage changes\n",
        "                q_change = torch.mean(torch.abs(fine_tuned_q_weights - base_q_weights) / torch.abs(base_q_weights + 1e-10)).item() * 100\n",
        "                k_change = torch.mean(torch.abs(fine_tuned_k_weights - base_k_weights) / torch.abs(base_k_weights + 1e-10)).item() * 100\n",
        "                v_change = torch.mean(torch.abs(fine_tuned_v_weights - base_v_weights) / torch.abs(base_v_weights + 1e-10)).item() * 100\n",
        "                o_change = torch.mean(torch.abs(fine_tuned_o_weights - base_o_weights) / torch.abs(base_o_weights + 1e-10)).item() * 100\n",
        "\n",
        "                q_changes.append(q_change)\n",
        "                k_changes.append(k_change)\n",
        "                v_changes.append(v_change)\n",
        "                o_changes.append(o_change)\n",
        "\n",
        "            return (q_changes, k_changes, v_changes, o_changes)\n",
        "            # Plot Q, K, V, O changes\n",
        "            # plt.figure(figsize=(10, 6))\n",
        "            # plt.plot(range(num_layers), q_changes, marker='o', color='blue', label=\"Q Projection\")\n",
        "            # plt.plot(range(num_layers), k_changes, marker='o', color='green', label=\"K Projection\")\n",
        "            # plt.plot(range(num_layers), v_changes, marker='o', color='red', label=\"V Projection\")\n",
        "            # plt.plot(range(num_layers), o_changes, marker='o', color='purple', label=\"O Projection\")\n",
        "            # plt.title(\"Percentage Change in Q, K, V, and O Projections Across Layers (LLaMA)\")\n",
        "            # plt.xlabel(\"Layer\")\n",
        "            # plt.ylabel(\"Percentage Change (%)\")\n",
        "            # plt.grid(True)\n",
        "            # plt.legend()\n",
        "            # plt.show()"
      ],
      "metadata": {
        "id": "-mS74OVzuQyq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8BLYEGwuXy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}